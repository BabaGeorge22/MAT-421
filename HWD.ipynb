{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMflgkoZKFjs6Wbhwb38Woh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BabaGeorge22/MAT-421/blob/main/HWD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module D Homework – George Tome**"
      ],
      "metadata": {
        "id": "vhtTLDsYizc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sections 1.1 - 1.3**"
      ],
      "metadata": {
        "id": "86qJRAzPi4RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 – Basic Linear Algebra"
      ],
      "metadata": {
        "id": "NOTejeW3jGYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In data science and machine learning, we work with large amounts of data organized into vectors. Linear algebra provides the mathematical framework for handling these structures efficiently. linear algebra is important because:\n",
        "\n",
        "\n",
        "***Data Representation***\n",
        "\n",
        "A dataset with\n",
        "n examples (rows) and\n",
        "d features (columns) can be stored in an\n",
        "n×d matrix. Each row is a data point (vector), and each column is a feature across all data points.\n",
        "\n",
        "\n",
        "***Transformations and Models***\n",
        "\n",
        "\n",
        "Many algorithms can be seen as matrix transformations. For example,\n",
        "Ax can describe a linear model, where\n",
        "x is the parameter vector.\n",
        "\n",
        "\n",
        "***Dimensionality Reduction***\n",
        "\n",
        "\n",
        "Principal Component Analysis uses eigenvalues or Singular Value Decomposition to find directions of maximum variance.\n",
        "\n",
        "\n",
        "***Optimization***\n",
        "\n",
        "\n",
        "Problems like least squares rely on solving linear systems (\n",
        "A\n",
        "T\n",
        " Ax=A\n",
        "T\n",
        " b) to get the best fit. The speed and accuracy of these solutions are based by linear algebra algorithms."
      ],
      "metadata": {
        "id": "2QePD-YkjNpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " -"
      ],
      "metadata": {
        "id": "dDiFVIwIkXNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1.2 – Elements of Linear Algebra**"
      ],
      "metadata": {
        "id": "Km8zlPwGkVe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectors and Norms: A vector is just a list of real numbers, and its norm measures how large or long the vector is.\n",
        "\n",
        "Dot Products and Orthogonality: We check if two vectors point in similar directions by combining their elements.\n",
        "\n",
        "Matrices: A matrix organizes numbers in rows and columns and can act as a function that transforms a vector from one space to another.\n",
        "\n",
        "Invertible and Determinant: Some square matrices have an inverse that undoes the transformation."
      ],
      "metadata": {
        "id": "kEDLUVGakguY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "v = np.array([1, 2, 3])\n",
        "w = np.array([4, -1, 0])\n",
        "dot_vw = np.dot(v, w)\n",
        "print(\"Dot product v·w:\", dot_vw)\n",
        "\n",
        "A = np.array([[2, 1],\n",
        "              [0, 3]], dtype=float)\n",
        "\n",
        "print(\"Det(A) =\", np.linalg.det(A))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f1sfACBlDQ7",
        "outputId": "8c4b966b-f80a-4f8a-9584-c5b0b0c3e50e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product v·w: 2\n",
            "Det(A) = 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.dot(v, w) finds how similar v and w are.\n",
        "\n",
        "\n",
        "np.linalg.det(A) tells us if A is invertible (nonzero = invertible)."
      ],
      "metadata": {
        "id": "AqqrJujUlILv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "a5M2HbVflNkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1.3 – Linear Regression**"
      ],
      "metadata": {
        "id": "64Zwh_1alOtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overdetermined Systems: There are sometimes more equations than unknowns, leading to an approximate solution instead of an exact one.\n",
        "\n",
        "Least Squares: Pick the solution that makes the model's predictions as close as possible to the data in a minimum error sense.\n",
        "\n",
        "Matrix Approach: By treating data as a matrix, we quickly find parameters that minimize the overall error using standard linear algebra equtions."
      ],
      "metadata": {
        "id": "IQejVHpklR6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 1],\n",
        "              [2, 1],\n",
        "              [3, 2]], dtype=float)\n",
        "b = np.array([2, 3, 5], dtype=float)\n",
        "\n",
        "# Solve least squares by using normal equations\n",
        "x_sol = np.linalg.solve(A.T @ A, A.T @ b)\n",
        "print(\"Best-fit parameters:\", x_sol)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-kBkQxjlpA7",
        "outputId": "8f29618c-44fd-409d-d4ae-818fd30488cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best-fit parameters: [1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrange our data in matrix A, the outcomes in b, and solve for a parameter vector that fits the data best in a squared-error sense."
      ],
      "metadata": {
        "id": "1ImB45xgluj3"
      }
    }
  ]
}